= Manage your Containers with OpenShift - A starter guide
Dennis Deitermann (dennis@redhat.com), Lutz Lange (Container Security part)
:scrollbar:
:data-uri:
:toc: left
:numbered:
:icons: font

image::http://www.rhpet.de/pictures/forum_vie2017.jpg[Red Hat Forum 2017 - Vienna]

== Introduction into the Lab

In this Lab you will get a first impression of the OpenShift Container Platform. You can decide to do the *Getting started with the OpenShift CLI and the Webinterface* `or` the *Containers and Security Lab*.

Unfortunately there is too less time for both Labs.

Have fun and enjoy the Lab :-)

=== Lab Reference

You have seven VMs for your own use. Only the Gateway VM is reachable from the internet. All others are behind a reverse proxy or can only be accessed through the ssh gateway.

To get your external IP address for the Gateway VM, please goto the http://lab.rhpet.de[Seat-to-IP overview Page^] and search your Seat-id in the table.

[cols="3*", options="header"]
|===
| VM Name| internal FQDN | internal IP
| SSH Gateway & Proxy Server | gw.example.com | 192.168.0.250, Ports 22&80&443 are open
| Master | master.example.com | 192.168.0.100, Port 8443 is open
| Infranode | infranode.example.com | 192.168.0.101
| App Node 1 | node1.example.com | 192.168.0.102
| App Node 2 | node2.example.com | 192.168.0.103
| App Node 3 | node3.example.com | 192.168.0.104
| CloudForms | cf.example.com | 192.168.0.200, Port 80&443 are open
|===

[cols="3*", options="header"]
|===
| Name | Password | Role
| rhpet | ask the instructor | ssh user to connect to the gateway VM
| root | r3dh4t1! | root user for all VMs
| admin | r3dh4t1! | OSCP & CloudForms Administrator & Auth user for the Proxy
| marina | r3dh4t1! | Developer/User
| andrew  | r3dh4t1! | Developer/User
|=== 

=== How to access the Lab Environment

First login to the ssh gateway with the user `rhpet`:

To obtain your ip address from the Gateway http://lab.rhpet.de[here^].

Login into the ssh gateway with the user `rhpet` and the password you got from the instructor.

----
[user@yourhost ~]$ ssh rhpet@<your gateway ip address>

The authenticity of host '104.199.108.30 (104.199.108.30)' can't be established.
ECDSA key fingerprint is SHA256:bsDGeuXiG1zpM3RlsN+RlaAPRaDSi6Y/sJoBP2IXNqU.
ECDSA key fingerprint is MD5:5f:b2:e7:c4:05:c2:37:10:1c:1f:a8:32:a8:ca:5a:38.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '104.199.108.30' (ECDSA) to the list of known hosts.

----------------------------------------------------------
 Welcome to the Manage your Container with OpenShift Lab

 Have fun and enjoy the Red Hat Forum 2017 in Vienna :-)
----------------------------------------------------------

Last login: Mon Oct 23 01:07:02 2017 from ipbcc3d64f.dynamic.kabel-deutschland.de
[rhpet@gw ~]$
----

TIP: Alternatively, if you have no ssh client, you can use the shell-in-a-box service. It is running on the Gateway on port 443. But be aware that some web browsers do not support direct Copy & Paste (you can use right click and then *Paste from browser*). So it might be more convenient to use a normal ssh client. This link works if you have configured the proxy server: https://gw.example.com[SSH Login into the gateway VM^]. As an alternetive you can type in "https://<your gateway ip address>" in your browser.

Then get the power of root:
----
[rhpet@gw ~]$ su -
----
The root pw is `r3dh4t1!`

For HTTP & HTTPS connections we need to configure a Proxy in your Webbrowser. We tested it with Firefox.
Please go to `Settings` → `Advanced` → `Network` → `Settings`

image::http://www.rhpet.de/pictures/Firefox-Proxy.png[Firefox Proxy configuration]

Please use your gateway IP address, port 80 and check the checkbox at "Use this proxy server for all protocols".

Username for the Proxy is: `admin` +
Password for the Proxy is: `r3dh4t1!`

The proxy works fine if you see the https://master.example.com:8443/[OpenShift login screen^]. Please accept the selfsigned certificate.

== Getting started with the OpenShift CLI and the Webinterface

With the OpenShift Container Platform command line interface (CLI), you can create applications and manage OpenShift Container Platform projects from a terminal. The CLI is ideal in situations where you are:

* Working directly with project source code.

* Scripting OpenShift Container Platform operations.

* Restricted by bandwidth resources and cannot use the web console.

The CLI is available using the `oc` command:
----
$ oc <command>
----

=== Basic Setup and Login

The `oc login` command is the best way to initially set up the CLI, and it serves as the entry point for most users. The interactive flow helps you establish a session to an OpenShift Container Platform server with the provided credentials. The information is automatically saved in a CLI configuration file that is then used for subsequent commands.

Login into the master host and the login into OpenShift as `admin` user with the password `r3dh4t1!`:
----
[root@gw ~]# ssh master
Last login: Thu Jun  8 10:10:12 2017 from 192.168.0.250
----
 
----
[root@master ~]# oc login https://master.example.com:8443

Authentication required for https://master.example.com:8443 (openshift)
Username: admin
Password: r3dh4t1!
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default
    kube-system
    logging
    management-infra
    openshift
    openshift-infra

Using project "default".
----

You can log out of CLI using the `oc logout` command. But we don't do this now.

=== Projects

A project in OpenShift Container Platform contains multiple objects to make up a logical application.

Most oc commands run in the context of a project. The `oc login` selects a default project during initial setup to be used with subsequent commands. Use the following command to display the project currently in use:

----
[root@master ~]# oc project

Using project "default" on server "https://master.example.com:8443".
----

If you have access to multiple projects, use the following syntax to switch to a particular project by specifying the project name:
----
[root@master ~]# oc project default

Already on project "default" on server "https://master.example.com:8443".
----

The `oc status` command shows a high level overview of the project currently in use, with its components and their relationships, as shown in the following example:
----
[root@master ~]# oc status

In project default on server https://master.example.com:8443

https://docker-registry-default.cloudapps.example.com (passthrough) to pod port 5000-tcp (svc/docker-registry)
  dc/docker-registry deploys docker.io/openshift3/ose-docker-registry:v3.5.5.8
    deployment #1 deployed 5 months ago - 1 pod

svc/kubernetes - 172.30.0.1 ports 443, 53->8053, 53->8053

https://registry-console-default.cloudapps.example.com (passthrough) to pod port registry-console (svc/registry-console)
  dc/registry-console deploys registry.access.redhat.com/openshift3/registry-console:3.5
    deployment #1 deployed 4 months ago - 1 pod (warning: 1 restarts)

svc/router - 172.30.49.219 ports 80, 443, 1936
  dc/router deploys docker.io/openshift3/ose-haproxy-router:v3.5.5.8
    deployment #1 deployed 5 months ago - 1 pod

2 warnings identified, use 'oc status -v' to see details.
----

If you want to learn more about the `oc` command, please look at the following documentation: +
https://docs.openshift.com/container-platform/3.5/cli_reference/basic_cli_operations.html[Developer CLI Operations^] +
https://docs.openshift.com/container-platform/3.5/cli_reference/admin_cli_operations.html[Administrator CLI Operations^]

=== Verify Your OpenShift Environment

On the master host run `oc get nodes` to check the status of your OpenShift hosts:
----
[root@master ~]# oc get nodes

NAME                    STATUS                     AGE
infranode.example.com   Ready                      159d
master.example.com      Ready,SchedulingDisabled   159d
node1.example.com       Ready                      159d
node2.example.com       Ready                      159d
node3.example.com       Ready                      159d
----

Check if the installer has deployed the router and the registry containers:
----
[root@master ~]# oc get pods

NAME                       READY     STATUS    RESTARTS   AGE
docker-registry-1-5gvfn    1/1       Running   1          37m
registry-console-1-tbwwj   1/1       Running   1          138d
router-1-xq3r6             1/1       Running   8          159d
----

=== Configure OpenShift

In this section, you check the labels and do some intial configuration.

=== Labels

Labels are used to organize, group, or select API objects. For example, pods are "tagged" with labels, and then services use label selectors to identify the pods they proxy to. This makes it possible for services to reference groups of pods, even treating pods with potentially different containers as related entities.

Most objects can include labels in their metadata. So labels can be used to group arbitrarily-related objects; for example, all of the pods, services, replication controllers, and deployment configurations of a particular application can be grouped.

Labels are simple key/value pairs, as in the following example:
----
labels:
  key1: value1
  key2: value2
----

Consider:

* A pod consisting of an *nginx* container, with the label *role=webserver*.

* A pod consisting of an *Apache httpd* container, with the same label *role=webserver*.

A service or replication controller that is defined to use pods with the *role=webserver* label treats both of these pods as part of the same group.

=== Check Regions and Zones

We already labeled your nodes.

Check the labels of the nodes:
----
[root@master ~]# oc get nodes --show-labels

NAME                    STATUS                     AGE       LABELS
infranode.example.com   Ready                      159d      beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=infranode.example.com,region=infra,zone=infranodes
master.example.com      Ready,SchedulingDisabled   159d      beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master.example.com,region=master
node1.example.com       Ready                      159d      beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node1.example.com,region=primary,zone=east
node2.example.com       Ready                      159d      beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node2.example.com,region=primary,zone=west
node3.example.com       Ready                      159d      beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node3.example.com,region=primary,zone=north
----

You now have a running OpenShift environment across five hosts with one master and four nodes, divided into three regions: master, infra and primary and three zones: east, west and north.

Check that registry and router are running on the infranode:
----
[root@master ~]# oc get pods -o wide

NAME                       READY     STATUS    RESTARTS   AGE       IP              NODE
docker-registry-1-5gvfn    1/1       Running   1          38m       10.128.0.11     infranode.example.com
registry-console-1-tbwwj   1/1       Running   1          138d      10.128.0.12     infranode.example.com
router-1-xq3r6             1/1       Running   8          159d      192.168.0.101   infranode.example.com
----

As you can see, all infrastructure pods are running on the infranode, because we configured a default node selector for this.
Please have a look https://blog.openshift.com/deploying-applications-to-specific-nodes/[here^] if you want more information.

=== Registry

The Registry is a stateless, highly scalable server side application that stores and lets you distribute Container images.
OpenShift Container Platform can utilize any server implementing the Docker registry API as a source of images, including the Docker Hub, private registries run by third parties, and the integrated OpenShift Container Platform registry.

==== Integrated OpenShift Container Registry

OpenShift Container Platform provides an integrated container registry called OpenShift Container Registry (OCR) that adds the ability to automatically provision new image repositories on demand. This provides users with a built-in location for their application builds to push the resulting images.

Whenever a new image is pushed to OCR, the registry notifies OpenShift Container Platform about the new image, passing along all the information about it, such as the namespace, name, and image metadata. Different pieces of OpenShift Container Platform react to new images, creating new builds and deployments.

==== Check integrated Registry

In this lab scenario, infranode is the target for both the registry and the default router.

To check the URL of the docker registry run `oc status`:
----
[root@master ~]# oc status

In project default on server https://master.example.com:8443

https://docker-registry-default.cloudapps.example.com (passthrough) to pod port 5000-tcp (svc/docker-registry)
  dc/docker-registry deploys docker.io/openshift3/ose-docker-registry:v3.5.5.8
    deployment #1 deployed 5 months ago - 1 pod

svc/kubernetes - 172.30.0.1 ports 443, 53->8053, 53->8053

https://registry-console-default.cloudapps.example.com (passthrough) to pod port registry-console (svc/registry-console)
  dc/registry-console deploys registry.access.redhat.com/openshift3/registry-console:3.5
    deployment #1 deployed 4 months ago - 1 pod

svc/router - 172.30.49.219 ports 80, 443, 1936
  dc/router deploys docker.io/openshift3/ose-haproxy-router:v3.5.5.8
    deployment #1 deployed 5 months ago - 1 pod

1 warning identified, use 'oc status -v' to see details.
----

Test the status of the registry with the curl command to communicate with the registrys service port, `curl -v https://registry-console-default.cloudapps.example.com --insecure`.
----
[root@master ~]# curl -v https://registry-console-default.cloudapps.example.com --insecure | grep "Red Hat Container Registry"

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* About to connect() to registry-console-default.cloudapps.example.com port 443 (#0)
*   Trying 192.168.0.101...
* Connected to registry-console-default.cloudapps.example.com (192.168.0.101) port 443 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
* skipping SSL peer certificate verification
* SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
* Server certificate:
* 	subject: CN=registry-console-1-tbwwj
* 	start date: Jun 08 11:03:26 2017 GMT
* 	expire date: Mai 15 11:03:27 2117 GMT
* 	common name: registry-console-1-tbwwj
* 	issuer: CN=registry-console-1-tbwwj
> GET / HTTP/1.1
> User-Agent: curl/7.29.0
> Host: registry-console-default.cloudapps.example.com
> Accept: */*
> 
< HTTP/1.1 200 OK
< Content-Security-Policy: default-src 'self' 'unsafe-inline'; connect-src 'self' ws: wss:
< Transfer-Encoding: chunked
< Cache-Control: no-cache, no-store
< 
{ [data not shown]
var environment = {"page":{"title":"Red Hat Container Registry","connect":true},"hostname":"registry-console-1-tbwwj","os-release":{"NAME":"Red Hat Container Registry","ID":"registry","PRETTY_NAME":"Red Hat Container Registry"},"OAuth":{"URL":"https://master.example.com:8443//oauth/authorize?client_id=cockpit-oauth-client&response_type=token","ErrorParam":null,"TokenParam":null}};
100 42229    0 42229    0     0   212k      0 --:--:-- --:--:-- --:--:--  213k
* Connection #0 to host registry-console-default.cloudapps.example.com left intact
----

Everything seems fine :-)

=== Resource Management Lab

In this lab, you learn how to manage OpenShift Container Platform resources.

* *Manage Users, Projects, and Quotas*
+
In this section, you create projects and test the use of quotas and limits.

* *Create Services and Routes*
+
In this section, you manually create services and routes for pods and review the changes to a service when scaling an application.

* *Explore Containers*
+
In this section, you run commands within active pods and explore the `docker-registry` and `Default Router` containers.

==== Manage Users, Projects, and Quotas

===== Create Project

On the master host, run `oadm` to create and assign the administrative user `andrew` to a project:

----
[root@master ~]# oadm new-project resourcemanagement --display-name="Resources Management" \
--description="This is the project we use to learn about resource management" \
--admin=andrew  --node-selector='region=primary'

Created project resourcemanagement
----

[NOTE]
`andrew` can create his own project with the `oc new-project` command, an option you will experiment with later in this course. Note that defining the `--node-selector` is optional.

==== View Resources in Web Console

Now have a look at the web console.

. Open your web browser and go to https://master.example.com:8443[https://master.example.com:8443^]
+
[NOTE]
====
The web console could take up to 90 seconds to become available after a restart of the master.
====

. When prompted, type the username and password, as follows:
** *Username*: `andrew`
** *Password*: `r3dh4t1!`

. In the web console, click the *Resources Management* project.
+
[NOTE]
The project is empty because it has no apps. You change that as part of this lab. 

===== Apply Quota to Project

A resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per project. It can limit the quantity of objects that can be created in a project by type, as well as the total amount of compute resources and storage that may be consumed by resources in that project.

. On the master host create a quota definition file:
+
----
[root@master ~]# cat << EOF > quota.json
{
  "apiVersion": "v1",
  "kind": "ResourceQuota",
  "metadata": {
    "name": "test-quota"
  },
  "spec": {
    "hard": {
      "memory": "512Mi",
      "cpu": "20",
      "pods": "3",
      "services": "5",
      "replicationcontrollers":"5",
      "resourcequotas":"1"
    }
  }
}
EOF
----

. On the master host, do the following:
.. Run `oc create` to apply the file you just created:
+
----
[root@master ~]# oc create -f quota.json --namespace=resourcemanagement

resourcequota "test-quota" created
----

.. Verify that the quota exists:
+
----
[root@master ~]# oc get quota -n resourcemanagement

NAME         AGE
test-quota   11s
----

.. Verify the limits and examine the usage:
+
[tabsize=8]
----
[root@master ~]# oc describe quota test-quota -n resourcemanagement

Name:			test-quota
Namespace:		resourcemanagement
Resource		Used	Hard
--------		----	----
cpu			0	20
memory			0	512Mi
pods			0	3
replicationcontrollers	0	5
resourcequotas		1	1
services		0	5
----
+

. On the web console, click the *Resource Management* project.

. Click the *Resources* tab

. Click *Quota* for information about the quota set.

==== Apply Limit Ranges to Project

For quotas to be effective, you must create _limit ranges_. They allocate the maximum, minimum, and default memory and CPU at both the pod and container level. Deployments to projects with a quota set will fail, if there are no default limits set for containers and pods. Pod and Containers with no limits are called unbound and are forbidden to run in quota projects.

. Create the `limits.json` file:
+
----
[root@master ~]# cat << EOF > limits.json
{
    "kind": "LimitRange",
    "apiVersion": "v1",
    "metadata": {
        "name": "limits",
        "creationTimestamp": null
    },
    "spec": {
        "limits": [
            {
                "type": "Pod",
                "max": {
                    "cpu": "500m",
                    "memory": "750Mi"
                },
                "min": {
                    "cpu": "10m",
                    "memory": "5Mi"
                }
            },
            {
                "type": "Container",
                "max": {
                    "cpu": "500m",
                    "memory": "750Mi"
                },
                "min": {
                    "cpu": "10m",
                    "memory": "5Mi"
                },
                "default": {
                    "cpu": "100m",
                    "memory": "100Mi"
                }
            }
        ]
    }
}
EOF
----

. On the master host, run `oc create` against the `limits.json` file and the
 `resourcemanagement` project:
+
----
[root@master ~]# oc create -f limits.json --namespace=resourcemanagement

limitrange "limits" created
----

. Review your limit ranges:
+
----
[root@master ~]# oc describe limitranges limits -n resourcemanagement

Name:		limits
Namespace:	resourcemanagement
Type		Resource	Min	Max	Default Request	Default Limit	Max Limit/Request Ratio
----		--------	---	---	---------------	-------------	-----------------------
Pod		cpu		10m	500m	-		-		-
Pod		memory		5Mi	750Mi	-		-		-
Container	cpu		10m	500m	100m		100m		-
Container	memory		5Mi	750Mi	100Mi		100Mi		-
----

==== Test Quota and Limit Settings

NOTE: You are running commands as the Linux users `andrew` and `root` in a lab environment. As a user it is unusual to use the `oc` command directly on the master. It is common to install `oc` on your workstation or notebook. You can get the OpenShift client tools for your operating system https://docs.openshift.com/container-platform/3.5/cli_reference/get_started_cli.html[here^].

. Now we switch to the OS user `andrew` and login into OpenShift with the OpenShift user `andrew`. 

.. When prompted, type the username and password:
** *Username*: `andrew`
** *Password*: `r3dh4t1!`
+
----
[root@master ~]# su - andrew
[andrew@master ~]$ oc login https://master.example.com:8443 -u andrew
----

* The output is as follows:
+
----
Login successful.

You have one project on this server: "resourcemanagement"

Using project "resourcemanagement".
Welcome! See 'oc help' to get started.
----
+
NOTE: This lab shows you the manual, step-by-step method of creating each object. This is done only for educational purpose. There are easier ways to create deployments and all the required objects. The most powerful way to create apps on OpenShift is the `oc new-app` command, which is covered later in this lab.

. Create the `hello-pod.json` pod definition file:
+
----
[andrew@master ~]$ cat <<EOF > hello-pod.json
{
  "kind": "Pod",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-openshift",
    "creationTimestamp": null,
    "labels": {
      "name": "hello-openshift"
    }
  },
  "spec": {
    "containers": [
      {
        "name": "hello-openshift",
        "image": "openshift/hello-openshift:v1.5.1",
        "ports": [
          {
            "containerPort": 8080,
            "protocol": "TCP"
          }
        ],
        "resources": {
        },
        "terminationMessagePath": "/dev/termination-log",
        "imagePullPolicy": "IfNotPresent",
        "capabilities": {},
        "securityContext": {
          "capabilities": {},
          "privileged": false
        }
      }
    ],
    "restartPolicy": "Always",
    "dnsPolicy": "ClusterFirst",
    "serviceAccount": ""
  },
  "status": {}
}
EOF
----

===== Run Pod

Here, you create a simple pod without a _route_ or _service_:

Create and verify the `hello-openshift` pod:
----
[andrew@master ~]$ oc create -f hello-pod.json

pod "hello-openshift" created
----
Wait a few seconds until the pod is up and running. (~40 seconds are needed) You can use `oc get pods -w` to see it directly when the status is changing.
----
[andrew@master ~]$ oc get pods

NAME              READY     STATUS    RESTARTS   AGE
hello-openshift   1/1       Running   0          41s
----

Run `oc describe` for details on your pod:
----
[andrew@master ~]$ oc describe pod hello-openshift

Name:			hello-openshift
Namespace:		resourcemanagement
Security Policy:	restricted
Node:			node2.example.com/192.168.0.103
Start Time:		Tue, 25 Apr 2017 19:15:01 -0400
Labels:			name=hello-openshift
Status:			Running
IP:			10.130.0.2
Controllers:		<none>
Containers:
  hello-openshift:
    Container ID:	docker://2674481be26d544323fa637c1cc5ba36a5eaafd4707f7735b2620045c495cb07
    Image:		openshift/hello-openshift:v1.5.1
    Image ID:		docker-pullable://docker.io/openshift/hello-openshift@sha256:7ce9d7b0c83a3abef41e0db590c5aa39fb05793315c60fd907f2c609997caf11
    Port:		8080/TCP
    Limits:
      cpu:	100m
      memory:	100Mi
    Requests:
      cpu:		100m
      memory:		100Mi
    State:		Running
      Started:		Tue, 25 Apr 2017 19:15:39 -0400
    Ready:		True
    Restart Count:	0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ylt00 (ro)
    Environment Variables:	<none>
Conditions:
  Type		Status
  Initialized 	True
  Ready 	True
  PodScheduled 	True
Volumes:
  default-token-ylt00:
    Type:	Secret (a volume populated by a Secret)
    SecretName:	default-token-ylt00
QoS Class:	Guaranteed
Tolerations:	<none>
Events:
  FirstSeen	LastSeen	Count	From				SubobjectPath			Type		Reason		Message
  ---------	--------	-----	----				-------------			--------	------		-------
  2m		2m		1	{default-scheduler }						Normal		Scheduled	Successfully assigned hello-openshift to node2.example.com
  1m		1m		1	{kubelet node2.example.com}	spec.containers{hello-openshift}	Normal		Pulling		pulling image "openshift/hello-openshift:v1.5.1"
  1m		1m		1	{kubelet node2.example.com}	spec.containers{hello-openshift}	Normal		Pulled		Successfully pulled image "openshift/hello-openshift:v1.5.1"
  1m		1m		1	{kubelet node2.example.com}	spec.containers{hello-openshift}	Normal		Created		Created container with docker id 2674481be26d; Security:[seccomp=unconfined]
  1m		1m		1	{kubelet node2.example.com}	spec.containers{hello-openshift}	Normal		Started		Started container with docker id 2674481be26d
----

Test that your pod is responding with `Hello OpenShift`: (note that the root password for node1 is also `r3dh4t1!`)
----
[andrew@master ~]$ oc describe pod hello-openshift|grep IP:|awk '{print $2}'

10.130.0.4

[andrew@master ~]# ssh root@node1 'curl -s http://10.130.0.4:8080'

root@node1's password: r3dh4t1!
----

* This output denotes a correct response:
+
----
Hello OpenShift!
----

We must ssh into an other node, because we don´t have direct access to the pod network on the master node.

Delete all the objects in your `hello-pod.json` definition file, which, at this point, is the pod only:

----
[andrew@master ~]$ oc delete -f hello-pod.json

pod "hello-openshift" deleted
----

TIP: You can also delete a pod using the following command format: #oc delete pod <PODNAME>.

Create a new definition file that launches four `hello-openshift` pods:

----
[andrew@master ~]$ cat << EOF > hello-many-pods.json
{
  "metadata":{
    "name":"quota-pod-deployment-test"
  },
  "kind":"List",
  "apiVersion":"v1",
  "items":[
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-1",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.5.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-2",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.5.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-3",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.5.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-4",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.5.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    }
  ]
}
EOF
----

Create the items in the `hello-many-pods.json` file:

----
[andrew@master ~]$ oc create -f hello-many-pods.json

pod "hello-openshift-1" created
pod "hello-openshift-2" created
pod "hello-openshift-3" created
Error from server (Forbidden): pods "hello-openshift-4" is forbidden: exceeded quota: test-quota, requested: pods=1, used: pods=3, limited: pods=3
----

[NOTE]
Because you defined a quota before, `oc create` created three pods only instead of four.

Delete the object in the `hello-many-pods.json` definition file:

----
[andrew@master ~]$ oc delete -f hello-many-pods.json

pod "hello-openshift-1" deleted
pod "hello-openshift-2" deleted
pod "hello-openshift-3" deleted
Error from server (NotFound): pods "hello-openshift-4" not found
----

==== Create Services and Routes

As `andrew`, create a project called `scvslab`:

----

[andrew@master ~]$ oc new-project svcslab --display-name="Services Lab" --description="This is the project we use to learn about services"
----

The output looks like this:

----
Now using project "svcslab" on server "https://master.example.com:8443".

You can add applications to this project with the 'new-app' command. For example, try:

    $ oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-hello-world.git

to build a new hello-world application in Ruby.
----

Create the `hello-service.json` file:

----
[andrew@master ~]$ cat <<EOF > hello-service.json
{
  "kind": "Service",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-service",
    "labels": {
      "name": "hello-openshift"
    }
  },
  "spec": {
    "selector": {
      "name":"hello-openshift"
    },
    "ports": [
      {
        "protocol": "TCP",
        "port": 8888,
        "targetPort": 8080
      }
    ]
  }
}
EOF
----

Create the `hello-service` service:

----
[andrew@master ~]$ oc create -f hello-service.json

service "hello-service" created
----

Display the services that are running in the current project:

----
[andrew@master ~]$ oc get services

NAME            CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
hello-service   172.30.213.165   <none>        8888/TCP   5s
----

Examine the details of your service. Note the following:
** *Selector*: Describes which pods the service selects or lists.
** *Endpoints*: Displays all the pods that are currently listed (none in your current project).

----
[andrew@master ~]$ oc describe service hello-service

Name:			hello-service
Namespace:		svcslab
Labels:			name=hello-openshift
Selector:		name=hello-openshift
Type:			ClusterIP
IP:			172.30.213.165
Port:			<unset>	8888/TCP
Endpoints:		<none>
Session Affinity:	None
No events.
----

Create pods according to the `hello-many-pods.json` definition file:

----
[andrew@master ~]$ oc create -f hello-many-pods.json

pod "hello-openshift-1" created
pod "hello-openshift-2" created
pod "hello-openshift-3" created
pod "hello-openshift-4" created
----

Wait a few seconds and check the service again.

* The pods that share the label `name=hello-openshift` are all listed:

----
[andrew@master ~]$ oc describe service hello-service

Name:			hello-service
Namespace:		svcslab
Labels:			name=hello-openshift
Selector:		name=hello-openshift
Type:			ClusterIP
IP:			172.30.213.165
Port:			<unset>	8888/TCP
Endpoints:		10.1.2.2:8080,10.1.2.3:8080,10.1.3.2:8080 + 1 more...
Session Affinity:	None
No events.
----

Test that your service is working:

----

[andrew@master ~]$ oc describe service hello-service|grep IP:|awk '{print $2}'

172.30.18.176

[andrew@master ~]$ ssh root@node1 'curl -s http://172.30.18.176:8888'

root@node1's password: r3dh4t1!

Hello OpenShift!
----

==== Explore Containers and Routes

Next, take a look at the route and registry containers.

===== Create Applications As Examples

As `andrew`, create a project called `explore-example`:
----
[andrew@master ~]$ oc new-project explore-example --display-name="Explore Example" --description="This is the project we use to learn about connecting to pods"

Now using project "explore-example" on server "https://master.example.com:8443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.
----

Applying the same image as before, run `oc new-app` to deploy `hello-openshift`:
----
[andrew@master ~]$ oc new-app --docker-image=openshift/hello-openshift:v1.5.1 -l "todelete=yes"

--> Found Docker image fb15b0b (4 weeks old) from Docker Hub for "openshift/hello-openshift:v1.5.1"

    * An image stream will be created as "hello-openshift:v1.5.1" that will track this image
    * This image will be deployed in deployment config "hello-openshift"
    * Ports 8080/tcp, 8888/tcp will be load balanced by service "hello-openshift"
      * Other containers can access this service through the hostname "hello-openshift"
    * WARNING: Image "openshift/hello-openshift:v1.5.1" runs as the 'root' user which may not be permitted by your cluster administrator

--> Creating resources with label todelete=yes ...
    imagestream "hello-openshift" created
    deploymentconfig "hello-openshift" created
    service "hello-openshift" created
--> Success
    Run 'oc status' to view your app.
----

Verify that `oc new-app` has created a pod and the service.

----
[andrew@master ~]$ oc get svc

NAME              CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
hello-openshift   172.30.24.220   <none>        8080/TCP,8888/TCP   37s
----

Wait until the Conatiner Status is Running. (it takes minute)
----
[andrew@master ~]$ oc get pods

NAME                      READY     STATUS    RESTARTS   AGE
hello-openshift-1-g3xow   1/1       Running   0          2m

----

Expose the service and create a route for the application:
----
[andrew@master ~]$ oc expose service hello-openshift --hostname=explore.cloudapps.example.com

route "hello-openshift" exposed
----

Check if the route works fine:
----
[andrew@master ~]$ curl http://explore.cloudapps.example.com

Hello OpenShift!
----

Now it works without the ssh, because we have an external route to the container.

In a later section, you explore the `docker-registry` container. To save time, start an S2I build now to push an image into the registry:

----
[andrew@master ~]$ oc new-app https://github.com/openshift/sinatra-example -l "todelete=yes"

--> Found image 27e89d9 (4 weeks old) in image stream "ruby" in project "openshift" under tag "2.3" for "ruby"

    Ruby 2.3
    --------
    Platform for building and running Ruby 2.3 applications

    Tags: builder, ruby, ruby23, rh-ruby23

    * The source repository appears to match: ruby
    * A source build using source code from https://github.com/openshift/sinatra-example will be created
      * The resulting image will be pushed to image stream "sinatra-example:latest"
    * This image will be deployed in deployment config "sinatra-example"
    * Port 8080/tcp will be load balanced by service "sinatra-example"
      * Other containers can access this service through the hostname "sinatra-example"

--> Creating resources with label todelete=yes ...
    imagestream "sinatra-example" created
    buildconfig "sinatra-example" created
    deploymentconfig "sinatra-example" created
    service "sinatra-example" created
--> Success
    Build scheduled, use 'oc logs -f bc/sinatra-example' to track its progress.
    Run 'oc status' to view your app.
----

===== Connect to Default Router Container

Get back to root:
----
[andrew@master ~]$ exit
----

. As `root`, make sure to use the default project. Open a Shell into the container with `oc rsh`
 command along with the default router's pod name.

----
[root@master ~]# oc project default

Now using project "default" on server "https://master.example.com:8443".
----

----
[root@master ~]# oc get pods

NAME                      READY     STATUS    RESTARTS   AGE
docker-registry-1-26xs7    1/1       Running   9          28d
registry-console-1-tbwwj   1/1       Running   5          8d
router-1-xq3r6             1/1       Running   12         28d
----

----
[root@master ~]# oc rsh router-1-xq3r6 
----

This prompt is displayed:
----
sh-4.2$ 
----

You are now running `bash` inside the container.

. Do the following:
.. Run `id`.
.. Run `pwd` and `ls` and note the directory you are in.
.. Run `grep hello-openshift` on the `haproxy.config` file.
.. Run `cat haproxy.config` to have a look on your configuration file.
+
----
sh-4.2$ id

uid=1000020000 gid=0(root) groups=0(root),1000020000
----
+
----
sh-4.2$ pwd

/var/lib/haproxy/conf
----
+
----
sh-4.2$ ls

cert_config.map		 os_edge_http_be.map	     os_sni_passthrough.map
default_pub_keys.pem	 os_http_be.map		     os_tcp_be.map
error-page-503.http	 os_reencrypt.map	     os_wildcard_domain.map
haproxy-config.template  os_route_http_expose.map
haproxy.config		 os_route_http_redirect.map
----
+
----
sh-4.2$ grep hello-openshift haproxy.config 

backend be_http_explore-example_hello-openshift

sh-4.2$ ps -ef

UID         PID   PPID  C STIME TTY          TIME CMD
1000020+      1      0  0 21:33 ?        00:00:02 /usr/bin/openshift-router
1000020+    294      1  0 22:09 ?        00:00:00 /usr/sbin/haproxy -f /var/lib/
1000020+    298      0  0 22:09 ?        00:00:00 /bin/sh
1000020+    305    298  0 22:10 ?        00:00:00 ps -ef
----
.. Examine the haproxy.config more closely. This could look something like this like this:
+
[subs=+macros]
----
sh-4.2$ grep -A 40 hello-openshift haproxy.config | sed '/^ *$/d'

backend be_http_explore-example_hello-openshift
  mode http
  option redispatch
  option forwardfor
  balance leastconn
  timeout check 5000ms
  http-request set-header X-Forwarded-Host %[req.hdr(host)]
  http-request set-header X-Forwarded-Port %[dst_port]
  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }
  http-request set-header X-Forwarded-Proto https if { ssl_fc }
  cookie 7cf54b74789cba0ee0faded0db7f5e0f insert indirect nocache httponly
  http-request set-header Forwarded for=%[src];host=%[req.hdr(host)];proto=%[req.hdr(X-Forwarded-Proto)]
pass:quotes[  *server*] 456a8f857d60f0a14165ad58cff18e10 10.128.2.32:8080 check inter 5000ms cookie 456a8f857d60f0a14165ad58cff18e10 weight 100
----
+
You see that you have only one endpoint defined. (The line which starts with server)
+
.. Exit the bash in the container to return to the root@master shell
+
----
sh-4.2$ exit

[root@master ~]# _
----
. As `andrew`, scale `hello-openshift` to have five replicas of its pod:
+
----
[root@master ~]# su - andrew
----
+
----
[andrew@master ~]$ oc get deploymentconfig

NAME              REVISION   REPLICAS   TRIGGERED BY
hello-openshift   1          1          config,image(hello-openshift:v1.5.1)
sinatra-example   1          1          config,image(sinatra-example:latest)
----
+
----
[andrew@master ~]$ oc scale dc hello-openshift --replicas=5

deploymentconfig "hello-openshift" scaled
----

. As `root` go back to the router container and view the `haproxy.config` file again:
+
[subs=+macros]
----
[andrew@master ~]$ exit
----
+
----
[root@master ~]# oc rsh router-1-xq3r6
----
+
----
sh-4.2$ grep -A 70 hello-openshift haproxy.config | sed '/^ *$/d'

backend be_http_explore-example_hello-openshift
  mode http
  option redispatch
  option forwardfor
  balance leastconn
  timeout check 5000ms
  http-request set-header X-Forwarded-Host %[req.hdr(host)]
  http-request set-header X-Forwarded-Port %[dst_port]
  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }
  http-request set-header X-Forwarded-Proto https if { ssl_fc }
  cookie 7cf54b74789cba0ee0faded0db7f5e0f insert indirect nocache httponly
  http-request set-header Forwarded for=%[src];host=%[req.hdr(host)];proto=%[req.hdr(X-Forwarded-Proto)]
pass:quotes[  *server* 456a8f857d60f0a14165ad58cff18e10 10.128.2.32:8080 check inter 5000ms cookie 456a8f857d60f0a14165ad58cff18e10 weight 100
  *server* 465c8af937146549fb2d68aa3adfde77 10.128.2.36:8080 check inter 5000ms cookie 465c8af937146549fb2d68aa3adfde77 weight 100
  *server* a19dc1b5f57a5cfe76f752ad8aa6c3a5 10.130.0.20:8080 check inter 5000ms cookie a19dc1b5f57a5cfe76f752ad8aa6c3a5 weight 100
  *server* 111eec0d645bb0897b3a9425563167b9 10.131.0.18:8080 check inter 5000ms cookie 111eec0d645bb0897b3a9425563167b9 weight 100
  *server*] aa8e80663b91a03be37ee9d33c3bc9c5 10.131.0.19:8080 check inter 5000ms cookie aa8e80663b91a03be37ee9d33c3bc9c5 weight 100
----

* All of your pods within the `haproxy` configuration are listed.

NOTE: Remember, the router routes proxy connections to the pods directly and not through the service. The router uses the service only to obtain a list of the pod endpoints (IP addresses).

Leave the container an switch to andrew:
----
sh-4.2$ exit

exit
[root@master ~]# su - andrew
----

==== Explore Registry Container

There are two containers that deal with registry related services. There is the docker-registry and there is the registry-console. We are looking at the docker-registry in this section. We will take a quick look at the https://registry-console-default.cloudapps.example.com[Registry-Console^] at a later time.

Please ensure that your build from earlier is complete.

. As user `*andrew*`, check the logs of the build that we stared a while back:
+
----

[andrew@master ~]$ oc logs builds/sinatra-example-1

Cloning "https://github.com/openshift/sinatra-example" ...
	Commit:	ff65a82271fffc60d4129bccde9c42ded49a199d (Merge pull request #11 from corey112358/patch-1)
	Author:	Ben Parees <bparees@users.noreply.github.com>
	Date:	Wed Jul 22 00:20:36 2015 -0400

---> Installing application source ...
---> Building your Ruby application from source ...
---> Running 'bundle install --deployment --without development:test' ...
Fetching gem metadata from https://rubygems.org/..........
Fetching version metadata from https://rubygems.org/..
Installing rack 1.6.0
Installing rack-protection 1.5.3
Installing tilt 1.4.1
Installing sinatra 1.4.5
Using bundler 1.10.6
Bundle complete! 1 Gemfile dependency, 5 gems now installed.
Gems in the groups development and test were not installed.
Bundled gems are installed into ./bundle.
---> Cleaning up unused ruby gems ...


Pushing image 172.30.17.242:5000/explore-example/sinatra-example:latest ...
Pushed 0/5 layers, 3% complete
Pushed 1/5 layers, 24% complete
Pushed 2/5 layers, 43% complete
Pushed 3/5 layers, 75% complete
Pushed 3/5 layers, 98% complete
Pushed 4/5 layers, 98% complete
Pushed 5/5 layers, 100% complete
Push successful
----
+
Notice the last few lines here. The *Push successful* indicates that the new container image was put into your internal registry.
+
. As `root`, start a shell inside the Container Context by running `oc rsh` along with the `docker-registry` pod name:
+
----
[root@master ~]# oc rsh docker-registry-1-<your registry id>
----

. Do the following:
.. Run `id`.
.. Run `pwd` and `ls` and note the directory you are in.
.. Run `cat config.yml`  to verify your configuration file.
+
----
sh-4.2$ id

uid=1000010000 gid=0(root) groups=0(root),1000010000
----
+
----
sh-4.2$ pwd

/
----
+
----
sh-4.2$ ls

bin   config.yml  etc	lib    media  opt   registry  run   srv  tmp  var
boot  dev	  home	lib64  mnt    proc  root      sbin  sys  usr
----
+
----
sh-4.2$ cat config.yml

version: 0.1
log:
  level: debug
http:
  addr: :5000
storage:
  cache:
    blobdescriptor: inmemory
  filesystem:
    rootdirectory: /registry
  delete:
    enabled: true
auth:
  openshift:
    realm: openshift

    # tokenrealm is a base URL to use for the token-granting registry endpoint.
    # If unspecified, the scheme and host for the token redirect are determined from the incoming request.
    # If specified, a scheme and host must be chosen that all registry clients can resolve and access:
    #
    # tokenrealm: https://example.com:5000
middleware:
  registry:
    - name: openshift
  repository:
    - name: openshift
      options:
        acceptschema2: false
        pullthrough: true
	mirrorpullthrough: true
        enforcequota: false
        projectcachettl: 1m
        blobrepositorycachettl: 10m
  storage:
    - name: openshift
----
+
. View the repositories and images that are available:
+
----
sh-4.2$ cd /registry/docker/registry/v2/repositories
----
+
----
sh-4.2$ ls

explore-example
----
+
----
sh-4.2$ ls explore-example/sinatra-example/_layers/

sha256
----
+
----
sh-4.2$ ls explore-example/sinatra-example/_layers/sha256/

02cbff0982e427fee158df11d35632f38410ee7e8b48212e681ecf3e60660ce4
5a865e48f2fdb4c48700b9aa800ecd8d0aff8611bec51fb4ab0f70ba09a0fb8e
89af3ab0c8b470502e9ed73ce6fa83f97e89a033f2553e9ba4e8a153c52a6373
9cc048a8a74a05eabd2f114d56d759435b8e2d76091e40edbff1d137b08de613
a778b52f148e84ec73f4ad7f7a1e67690dd0a36ddf1ed2926ad223901d196bf7
d65e4475a277c626c504de9433b98c30350e4cb940feb858b8563a6031e809a5
----
+
. As user `andrew`, look at one of the pods you started earlier:
+
----
[andrew@master ~]$ oc get pods

NAME                      READY     STATUS      RESTARTS   AGE
hello-openshift-1-4ywxh   1/1       Running     0          7m
hello-openshift-1-5vsyl   1/1       Running     0          7m
hello-openshift-1-9ivns   1/1       Running     0          19m
hello-openshift-1-byte3   1/1       Running     0          7m
hello-openshift-1-riupx   1/1       Running     0          7m
sinatra-example-1-build   0/1       Completed   0          17m
sinatra-example-1-ebuiu   1/1       Running     0          14m
----

. Connect to the container:
+
----
[andrew@master ~]$ oc exec -ti sinatra-example-1-ebuiu "/bin/bash"

bash-4.2$
----

. Explore the container:
.. Run `id`.
.. Run `pwd` and `ls` and note the directory you are in.
.. Run `ps -ef` to see what processes are running.
+
----

bash-4.2$ id

uid=1000060000 gid=0(root) groups=0(root),1000060000

bash-4.2$ pwd

/opt/app-root/src

bash-4.2$ ls

Gemfile       README.md  config.ru	  example-mustache	 public
Gemfile.lock  app.rb	 example-model	  example-views		 tmp
README	      bundle	 example-modular  example-views-modular

bash-4.2$ ps -ef

UID         PID   PPID  C STIME TTY          TIME CMD
1000050+      1      0  0 22:41 ?        00:00:01 ruby /opt/app-root/src/bundle/
1000050+     33      0  0 22:51 ?        00:00:00 /bin/bash
1000050+     62     33  0 22:51 ?        00:00:00 ps -ef
----
+
[NOTE]
Your pod names and output differ slightly.

=== Creating Applications Lab

This lab includes the following sections:

* *Deploy Application on Web Console*
+
In this section, you deploy an application from a code repository and follow the build logs on the OpenShift Container Platform web console and CLI.

* *Customize Build Script*

- Create an application from a forked Git repository, inject a custom build script, and start a rebuild from the web console.

- Review your custom script messages in the logs.

==== Deploy Application on Web Console

Here, you connect to and become familiar with the web console, create a project and an application, and scale a deployment and the topology view.

===== Connect To and Explore Web Console

. Use your browser to go to the OpenShift web console at `https://master.example.com:8443[https://master.example.com:8443^]`.

. Log in as `andrew` with the password `r3dh4t1!`.

. Take a few minutes to browse your projects.

===== Create New Project

. Click *Projects* and select *View all projects* to return to the Projects view.

. Click the blue *New Project* button in the top right corner.

. Give the new project a name, display name, and description:
* *Name*: `my-ruby-project`
* *Display Name*: `My Ruby Example Project`
* *Description*: An explanation of your choice

Once the project is in place, the *Add to Project* screen is displayed.

==== Create New Application

. In the *Add to Project* screen, type `ruby` in the search field of the *Browse Catalog* Tab to filter the available instant apps, templates, and builder images.

. We choose the plain Ruby Application here
. Set the version to `2.2` 
. Click "Select"

. Specify the name and Git repository URL:
* *Name*: `my-ruby-hello-world`.
* *Git Repository URL*: `https://github.com/openshift/ruby-hello-world`.

. Click *Show advanced options for source, routes, builds, and deployments.* and select the following options:
.. Notice that you get a route per default for your application.
.. Note that you can decide if Builds or Deployments should start automatically.
.. Change the scaling parameter to 3.
.. Create a label for app by the name of `environment` and the value of `dev`.

. Accept and create the application.

. Click *Continue to Overview* to go to the application's *Overview* screen.

. Click *View Log* to verify that a build is in progress. (this needs some time)

. Review the log as the build progresses.

. Wait for the build to complete and use a browser to navigate to the
 application route: http://my-ruby-hello-world-my-ruby-project.cloudapps.example.com[http://my-ruby-hello-world-my-ruby-project.cloudapps.example.com^]
//.. The database for our application isn't running, so expect to see the web
// page mention that.
+
[TIP]
====
* You can also use the command line to create a new application: `oc new-app https://github.com/openshift/ruby-hello-world -l  environment=dev`.

* To change scaling from the command line, use `oc scale`.
====

==== Scale Deployment 

. Go back to your application's *Overview* screen by clicking *Overview* at the upper left side.

. Observe the circle that shows the current number of pods, which is 3. You can increase that number by clicking the `^` button next to it.

. Click the `^` button twice to increase the number of replicas to 5.

. Go to *Applications* and select *Pods* to take a look at your new pods.

. Go back to your application's *Overview* screen by clicking *Overview* again.


=== Templates Lab

This lab includes the following sections:

* *Create and Upload Template*
+
In this section, you create a template for a two-tier application (front end and database), upload it into the shared namespace (the `openshift` project), and ensure that users can deploy it from the web console.

* *Use Templates and Template Parameters*
+
In this section, you create two separate template instances in two separate projects and establish a front-end-to-database-back-end connection by means of template parameters.

[NOTE] 
.Templates are a complex 
====
Templates allow an easy way to define all the required objects of an complex to be sepcified together and made available in Catalogs. Please see our link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-templates[OpenShift Documentation on Templates^] for more information.
====

==== Create and Upload Template

===== Install Template

The example in this section shows an application and a service with two pods: a front-end web tier and a back-end database tier. This application uses auto-generated parameters and other sleek features of OpenShift Container Platform.  Note that this application contains predefined connectivity between the front-end and back-end components as part of its YAML definition. You add further resources in a later lab.

This example is, in effect, a "quick start" -- a predefined application that comes in a template and that you can immediately use or customize.

. As `root` on the master host, download the template's definition file:
+
----
[root@master ~]# wget http://people.redhat.com/~llange/yaml/Template_Example.yml
----

. Create the template object in the shared `openshift` project. This is also referred to as _uploading_ the template.
+
----
[root@master ~]# oc create -f Template_Example.yml -n openshift

template "a-quickstart-keyvalue-application" created
----
NOTE: The `Template_Example.yml` file defines a template. You just added it to the openshift project. This make your template available throughout your OpenShift cluster. If you want to just have this temlate available for certain projects, put it directly into the project namespace and refrain from adding it to the `openshift` project.

The OpenShift Container Platform comes with a long list of preconfigured templates available for usage. You can take a look at the installed list with the following `oc` command. This list had 117 entries, that is why we did not include the output here. 

----
[root@master ~]# oc get templates -n openshift 

... <many lines> ...
sso70-postgresql-persistent                     Application template for SSO 7.0 PostgreSQL applications with persistent storage   33 (17 blank)     8
----

Do not be alarmed by the complexity of Templates. You can even create templates from existing Objects. Please see our Documentation on 
link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#export-as-template[How to Create a Template from existing Objects^].

===== Create Instant App from Template

. On your browser, connect to the OpenShift web console at `https://master.example.com:8443[https://master.example.com:8443]`:
.. Log in as `andrew` with the password `r3dh4t1!`.

. Click the blue *New Project* button in the top right corner.

. Specify the project name, display name, and description:
* *Name*: `instant-app`
* *Display Name*: `instant app example project`
* *Description*: `A demonstration of an instant app or template`.
+
[TIP]
====
Alternatively, perform this step from the command line:
----
[root@master ~]# oadm new-project instant-app --display-name="instant app example project" --description='A demonstration of an instant-app/template' --node-selector='region=primary' --admin=andrew
----
====

. From the `instant-app` project's *Overview* screen, click *Add to project*.
+
. Click the `ruby` tile to display ruby based applications and builder images
+
[NOTE]
Here you find the instant application, a special kind of template with the `instant-app` tag. The idea behind an instant application is that, when you create a template instance, you already have a fully functional application. In this example, your instant application is just a simple web page for key-value storage and retrieval.
+
. Select *a-quickstart-keyvalue-application*.
+
The template configuration screen is displayed. Here, you can specify certain options for instantiating the application components:
+
.. Set the `ADMIN_PASSWORD` parameter to your favorite password.
.. Add a label named `version` with the value `1`.

. Click *Create* to instantiate the services, pods, replication controllers, etc.

* The build starts immediately.
. Wait for the build to finish. You can browse the build logs to follow the progress.

[NOTE]
Our Application is currently still missing heath checks for all containers. You will deal with health checks later in this lab. If you are an experienced OpenShift User feel free to build a template with health checks included.

===== Use Application

After the build is complete and both frontend and database are up and running, visit your application at `http://example-route-instant-app.cloudapps.example.com/[http://example-route-instant-app.cloudapps.example.com/^]`.

[NOTE]
Be sure to use HTTP and _not_ HTTPS. HTTPS does not work for this example because the form submission was coded with HTTP links.

== Containers and Security Lab

In this Lab you will:

* Start a Ruby based example application
* Add a Heath Check
* Scan the container image for known vulnerabilities
** [Optional : prevent execution of images with know vulnerabilities]
* Patch issues found if possible
* Create a schedule for scanning container images 
* Configure OpenShift to always get the latest ruby builder image

=== CloudForms Introduction

CloudForms is the designated Operations Tool for the Openshift Container Platform. But CloudForms is much more than just a tool to look at and manage OpenShift. It originally found it's way into the Red Hat portfolio though the acquisition of the company ManageIQ. It was primarily a virtualisation management tool in the beginning. The big differentiator to other existing tools was the main focus on *Operational Visibility* or *Insight* as it is called back in the day.

CloudForms is a manager of managers. It talks to the APIs of other management infrastructures. These are called providers.

.CloudForms can be the central manager for all these infrastructures
* AWS
* Google Cloud
* Azure
* Red Hat OpenStack
* Microsoft System Center VMM
* Red Hat Virtualization
* VmWare vCenter
* Ansible Tower
* *Red Hat OpenShift Container Platform*

image::http://people.redhat.com/~llange/labimg/CloudForms-Overview1.png[]

=== Connect to CloudForms

We did deploy a CloudForms 4.5 for you as part of this Lab. Open your Browser and connect to it via https://cf.example.com[https://cf.example.com^]. 

. Log into the CloudForms Interface using the User `admin` and the password `r3dh4t1!`. 
+
You will find the main navigation panel on the right hand side. Hover over `Compute`, move to `Containers` and Click on `Overview` in the 3rd side panel. This will bring you to the Container Dashboard. This is an Overview over all configured OpenShift environments.
+
image::http://people.redhat.com/~llange/labimg/CloudForms-Container-Provider-Dashboardv2.png[Container Dashboard]
+
The container dashboard give a quick overview of the known / configured OpenShift Cluster Environments. The section at the top of the board lists the number of known Objects. Below this are several usage statistics. These are filled only if the hawkular metric stack is set up in your OpenShift Container Platform. Note that it will take up to 24h after configuring the Hawkular part of the provider setup in CloudForms until the usage information is displayed.
+
CloudForms offers another tool called *Topology*. This view might be familiar to you if you know OpenStack Horizon. The Topology view can be quite full and overwhelming if your cluster is bigger or has many applications.  
+
. Go to Compute -> Containers -> Click on Topology.
+
image::http://people.redhat.com/~llange/labimg/CloudForms-Container-Topology.png[]
+
If this view is too full use the service icons to toggle visibilty of the respective objects. You could also use the search field to grey out every object not matching your search. 
+
The nice thing about the topology view is that every object is displayed with a status indicator. In our case every object has a green border. It an object has a failed state, you will see it with a red boarder instead. You could chose to display object names, or hover over the object with the mouse cursor to see name, type and status of that object. A double click on the object will bring you to the details page of that object in CloudForms.

=== Application Setup

For more background information on Application setup consult the official link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-new-app[OpenShift Documentation here^].

There are multiple ways to start or create your application in OpenShift. You can use the oc tool from the command line, or you can use the WebUI. You could even do it with the RestAPI. We will document how to use the command line to create a test application here. You are free to use the Web UI as well. The command line offers a powerful oc sub command called new-app. *oc new-app* is the swiss army knife for application creation as it will create all the objects you need to run your application in the OpenShift Container Platform. 

You will first create a project for you application to live in. Projects are used to separate Application Management. There can be multiple apps with the same name on the same OpenShift cluster, as long as they live in different projects. 

. Create a project called "testproject" as user andrew now :
+
----
[andrew@master ~]$ oc new-project testproject --description="My Test Project" --display-name="Test Project"

now using project "testproject" on server "https://master.example.com:8443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.
----
+
You can use ImageStreams, Templates and Docker Images to create an Application.
+
An *image stream* comprises any number of Docker-formatted container images identified by tags. It presents a single virtual view of related images, similar to an image repository. Image streams can be used to automatically perform an action when new images are created. Builds and deployments can watch an image stream to receive notifications when new images are added and react by performing a build or deployment, respectively.
+
A *template* describes a set of objects that can be parameterized and processed to produce a list of objects for creation by OpenShift Container Platform. The objects to create can include anything that users have permission to create within a project, for example services, build configurations, and deployment configurations. A template may also define a set of labels to apply to every object defined in the template.
+
Be aware that the *Docker Container images* need to be compatible with the OpenShift security restrictions to run in the platform. Most images found on Docker Hub do not adhere to security best practices and run as root. This is not allowed on a default OpenShift installation. Take a good look at the link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/creating_images/#creating-images-guidelines[OpenShift Guidelines for Container Images^]. Be sure to look through the link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/creating_images/#openshift-container-platform-specific-guidelines[OpenShift Specific Guidelines^] as well.
+
. Create a Test Application using the ruby:2.3 builder image and the ruby-ex example application. You can do this in the WebUI or on the command line. This is how to do it on the command line : 
+
----
[andrew@master ~]$ oc new-app openshift/ruby:2.3~https://github.com/openshift/ruby-ex --name=rtest
----
+
. Go to the Overview Page of your testproject in the Web UI. If you are quick enough, you will be able to see the following screen. Notice that there is a build in progress.
+
image::http://people.redhat.com/~llange/labimg/TestProject-rtest1-Deploy-1.png[]
+
. Click "View Log" and go to the build page. You can view the logs of the build process here. Notice that the last line should read "Push successful". This tells us that the resulting image is saved in internal Registry.
+
image::http://people.redhat.com/~llange/labimg/TestProject-rtest1-buildlog.png[]
+
. After successful deployemnt your Overview page of the Test Project should look like this :
+
image::http://people.redhat.com/~llange/labimg/TestProject-rtest1-no-route.png[]
+
You will need to create a route object to expose you application to access from the outside. Note that there will be a Route for your application already if you created it using the Web UI. If there is no route for you application you will find the "Create Route" button in top right corner like in the screen shot above. You could use this button to create a route in the Web UI. Or you could expose you application on the command line with :
+
[source,cmd,indent=o]
----
[andrew@master ~]# oc get service

NAME      CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
rtest     172.30.111.54   <none>        8080/TCP   20h
----
+
----
[andrew@master ~]# oc expose service rtest

route "rtest" exposed
----
+
[source,cmd,indent=o]
----
[andrew@master ~]# oc get route

NAME      HOST/PORT                                 PATH      SERVICES   PORT       TERMINATION   WILDCARD
rtest     rtest-testproject.cloudapps.example.com             rtest      8080-tcp                 None
----
+
You Overview page of the Test Project should now display the URL link:http://rtest-testproject.cloudapps.example.com[http://rtest-testproject.cloudapps.example.com^] instead of the "Create Route" button. Click the link to go see if your application is working. The result should look like this :
+
image::http://people.redhat.com/~llange/labimg/TestProject-rtest1-the-app.png[]
+
If something went wrong and you want to delete your application, you can do this with the oc tool using the label app=truby. *Do not delete the app* if it runs without problems, we will use it in the next section of this lab. 
+
. Delete the app with `oc delete all -l app=rtest`, if you want to start over and try the oc new-app command again in step 2.

=== Application Health Checks

Read more about Readiness and Liveness Check in the link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-application-health[OpenShift Developer Guide about health checks^].

It is good style for your application to provide health information for the platform to consume. A best practice pattern is to offer a web page that provides a good return code if your app is healthy. The ruby example that we used provides this information here : link:http://rtest-testproject.cloudapps.example.com/health[http://rtest-testproject.cloudapps.example.com/health^]. 

You can take a look at the application source on Github.com : https://github.com/openshift/ruby-ex[https://github.com/openshift/ruby-ex^] for more details.

. Got to the Web UI and open the Overview page for your testproject. Notic that you are displayed a warning about missing health checks. 
+
image::http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-1.png[]
+
. Add the health check to your deployement by clicking "Add Health Checks" and then "Add Liveliness Probe".
+
image::http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-2-App-Probe.png[]
+
There are three possible types of checks that you can chose. HTTP Get is suitable for our application here. Don't forget to add the path "/health" before you hit the "save" button. 
+
image::http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-3-add-probe-details.png[]
+
Notice that saving your changes changes your deployement settings, thus a new deployement is triggerd by the configuration change. 
+
image::http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-4-new-deployment-list.png[]
+
If you are quick enough, you can see the new deployment happening live.
+
. Go to the Overview Page of your "Test Project"
+
image::http://people.redhat.com/~llange/labimg/OpenShift-Missing-Health-Checks-5-new-deployement-overview.png[]
+
The platform is now able to even detect internal application failure situations, but is of cause depending on the nature of the the faults in the application and the quality of the health checks implemented. If you do not provide any health checks, OpenShift falls back onto checking if the docker container is listed as up and running. 

=== Container Image Security Scans with CloudForms

Container Scans help you to determine known vulnerabilities in your container images. You can run Container Images scans from CloudForms. Initiate a Container Scan for your the container image of the "rtest" application. 

. Sign in to your CloudForms instance link:https://cf.example.com[https://cf.example.com^]. 
. Log in as User "admin" with the password "r3dh4t1!".
. Go to : Compute -> Containers -> Container Images
. Fill out the Search box in the upper right corner search for "rtest".
+
[NOTE]
.Can't find the image -> Initiate a Container Provider Refresh
[subs=+macros]
====
If you can't see your image, you can wait and try again later or you can initiate a Provider Refresh in CloudForms. 
+
. Go to "Compute" -> "Containers" -> "Providers" 
. Select the OpenShift Cluster 1. 
. Select the Configuration Menu and click "Refresh Items and Relationships". 
+
It might still take ~15 min for the Refresh to run.
+
image::http://people.redhat.com/~llange/labimg/CloudForms-Container-Provider-Refresh.png[] 
====
+
. In the search results page, click on the name field of the rtest container image to take a loot at the details. 
. Notice that there is no information in the Configuration Box about the RPM *packages*. 
. There is no OpenSCAP Results as well. And the Compliance Box tells us that there is no status available.
+
image::http://people.redhat.com/~llange/labimg/rtest-Image-details-before-scan.png[]
+
We want to change this. 
+
. Assign a Policy Profile first.
+
.. Open the Policy Menu
.. Click "Manage Policies"
.. Select the Policy Profile "OpenSCAP profile"
.. Click "save"
+
. You should see a box that reads "Policy Provile assigned successfully". 
. Request an analysis of image content, or in other words start a Container Image Scan.
+
.. Open the Configuration Menu
.. Select "Perform SmartState Analysis"
.. Confirm "Perform SmartState Analysis on the selected items"
+
Look for the scan in the Web UI
+
.. Go back to the OpenShift Web UI
.. Make sure you are loged in as `admin`
.. Select the "management-infra" project
.. You should see a workload starting for the container scan. Notice the openshift3/image-inspector image.
+
image::http://people.redhat.com/~llange/labimg/Image-Scan-in-progress.png[]
+
The scan and the transfer of the information takes a few minutes in our demo environment. After the scan you should be able to see the scan details after a reload of the page in the CloudForms Interface.
+
image::http://people.redhat.com/~llange/labimg/rtest-scan-results-overview.png[]
+
You should now see values like these :
+
====
* 430 Packages
* 458 OpenSCAP Results
* An OpenSCAP html listed as available
* The OpenSCAP Failed Rules Summary list 3 High severities results.
* The Image was marked a *Non-Compliant*
* We do have a Compliance History available now.
====
+
. Click on the line "OpenScap Scan Results"
. Click the String "Result" to sort for fails
. You will notice that the name of the rule that failed is no helpful information for us 
+
image::http://people.redhat.com/~llange/labimg/OpenSCAP-scan-results-in-cf-rtest.png[]
+
. Click on the OpenSCAP html line. Use your browser to display this html page.
. Deselect the box "pass" to see the failed rules quickly. This list has the Red Hat Security Advisory Numbers and short text in the Rule Overview section table under tiles.
+
image::http://people.redhat.com/~llange/labimg/rtest-scan-results-scap-html.png[]
+
Every Red Hat Security Advisory is explained in fine detail on https://rhn.redhat.com/errata[https://rhn.redhat.com/errata^] and https://access.redhat.com/errata[https://access.redhat.com/errata^]. The first page is the older incarnation that is still around and has more detail than the Customer Portal equivalent. You can go directly to https://access.redhat.com/errata/RHSA-2017:0372[https://access.redhat.com/errata/RHSA-2017:0372^] . 
+
====
.Links to the Customer Portal Advisory Pages for the issues found
. link:https://access.redhat.com/errata/RHSA-2017:0372[RHSA-2017:0372: kernel-aarch64 security and bug fix update (Important)^]
. link:https://access.redhat.com/errata/RHSA-2017:1308[RHSA-2017:1308: kernel security, bug fix, and enhancement update (Important)^]
. link:https://access.redhat.com/errata/RHSA-2017:1365[RHSA-2017:1365: nss security and bug fix update (Important)^]
====
+
Lets go through the list to see and evaluate what this scan found in our image.

==== RHSA-2017:0372
This is an issue that is effecting kernels on Arm Architectures. Red Hat provides updated packages for aarch64 to fix this. The scans that we are doing are based on package numbers. And our container image holds a kernel specific package as well.

. Check the package list of the Container Image for kernel packages.

.. You could loCloudForms, the WebUI or on the command line.

----
# oc project testproject
Now using project "testproject" on server "https://master.example.com:8443".
# oc get pods
NAME            READY     STATUS      RESTARTS   AGE
rtest-1-build   0/1       Completed   0          17h
rtest-1-rfs06   1/1       Running     0          17h
# oc rsh rtest-1-rfs06
sh-4.2$ rpm -qa | grep kernel
kernel-headers-3.10.0-514.16.1.el7.x86_64
----

As we are clearly not on aarch64. This is x86_64 so we can ignore this result and file a bugzilla against CloudForms and the Container Scanner. This is a false finding.

==== RHSA-2017:1308

This advisory relates to multiple issues with only one issue marked as important. A local attacker could possibly use a flaw in the packet_set_ring() function of the kernel to produce a buffer overflow and crash a system if that application ran with CAP_NET_RAW. It might be possible to use this buffer overflow to gain additional privileges.

The question to ask here is, if this is / might be a problem for the container that you are running. As the affected package here is only the kernel-headers package, we can assume that this needs to be checked and fixed on the container host side as well. 

[NOTE]
This issue will only impact you, if you did build an application on top of this image that used these kernel headers. This is why we have this advisory included. 

==== RHSA-2017:1365

The third issue listed here is found in the Name Service Switch. This issue effects the name service switch that is genuine installed as rpms in the image.

[quote, RHSA-2017:1365 and CVE-2017-7502]
____
A null pointer dereference flaw was found in the way NSS handled empty SSLv2 messages. An attacker could use this flaw to crash a server application compiled against the NSS library. (CVE-2017-7502)
____

==== Image Details

To be able to evaluate how to fix issues, you need to know where an Image came from and who created it. You should turn to the image creator first to fix issues found. There many different places that you can turn to for details of this image. 

. You can look at the detailed page for the rtest Container Image in CloudForms
. You can open the link:https://registry-console-default.cloudapps.example.com/registry#/images/testproject/rtest:latest[OpenShift Registry Console^] in OpenShift ( login as admin or andrew )
. You can got to the `oc` command line in OpenShift and look at Images and ImageStreams.

We are using the `oc` command here. Note the bold printed parts below :

----
$ oc get images | grep rtest
sha256:b925ddb55063d5f26526ca09e2f55aec5a8c4e95a7e4e4b644dd6ba08e3733c4   172.30.120.134:5000/testproject/rtest@sha256:b925ddb55063d5f26526ca09e2f55aec5a8c4e95a7e4e4b644dd6ba08e3733c4

----
 
OpenShift identifies images with sha256 values. So we have to use this sha256 value to take a closer look.

[subs=+macros]
----
$ oc describe image sha256:b925ddb55063d5f26526ca09e2f55aec5a8c4e95a7e4e4b644dd6ba08e3733c4
Name:		sha256:b925ddb55063d5f26526ca09e2f55aec5a8c4e95a7e4e4b644dd6ba08e3733c4
Namespace:	<none>
Created:	23 hours ago
Labels:		<none>
Annotations:	images.openshift.io/deny-execution=true <1>
		openshift.io/image.managed=true
		security.manageiq.org/failed-policy=openscap policy <2>
Docker Image:	172.30.120.134:5000/testproject/rtest@sha256:b925ddb55063d5f26526ca09e2f55aec5a8c4e95a7e4e4b644dd6ba08e3733c4
Image Size:	170.1 MB (first layer 73.86 MB, last binary layer 912.3 kB)
Image Created:	23 hours ago
Author:		<none>
Arch:		amd64
Entrypoint:	container-entrypoint
Command:	/usr/libexec/s2i/run
Working Dir:	/opt/app-root/src
User:		1001
Exposes Ports:	8080/tcp
Docker Labels:	architecture=x86_64
		authoritative-source-url=registry.access.redhat.com <3>
		build-date=2017-04-21T09:41:29.844044
		com.redhat.build-host=ip-10-29-120-102.ec2.internal
		com.redhat.component=rh-ruby23-docker
		description=The Red Hat Enterprise Linux Base image is designed to be a fully supported foundation for your containerized applications.  This base image provides your operations and application teams with the packages, language runtimes and tools necessary to run, maintain, and troubleshoot all of your applications. This image is maintained by Red Hat and updated regularly. It is designed and engineered to be the base layer for all of your containerized applications, middleware and utilites. When used as the source for all of your containers, only one copy will ever be downloaded and cached in your production environment. Use this image just like you would a regular Red Hat Enterprise Linux distribution. Tools like yum, gzip, and bash are provided by default. For further information on how this image was built look at the /root/anacanda-ks.cfg file.
		distribution-scope=public
		io.k8s.description=Platform for building and running Ruby 2.3 applications
		io.k8s.display-name=testproject/rtest-1:00972bc1
		io.openshift.build.commit.author=Ionut Palade <PI-Victor@users.noreply.github.com>
		io.openshift.build.commit.date=Mon Dec 12 14:37:32 2016 +0100
		io.openshift.build.commit.id=855ab2de53ff897a19e1055f7554c64d19e02c50
		io.openshift.build.commit.message=Merge pull request #6 from aj07/typo
		io.openshift.build.commit.ref=master
		io.openshift.build.image=registry.access.redhat.com/rhscl/ruby-23-rhel7@sha256:4b496b8b4d306badbea387f790004f867ca774526c17fb0fffdc88d58384c495 <4>
		io.openshift.build.source-location=https://github.com/openshift/ruby-ex.git
		io.openshift.expose-services=8080:http
		io.openshift.s2i.scripts-url=image:///usr/libexec/s2i
		io.openshift.tags=builder,ruby,ruby23,rh-ruby23
		io.s2i.scripts-url=image:///usr/libexec/s2i
		name=rhscl/ruby-23-rhel7 <5>
		release=6.7 <6>
		summary=Platform for building and running Ruby 2.3 applications
		vcs-ref=368e1c5301205f920e5a1ad00b075878d6cd3d54
		vcs-type=git
		vendor=Red Hat, Inc.
		version=2.3
Environment:	OPENSHIFT_BUILD_NAME=rtest-1
		OPENSHIFT_BUILD_NAMESPACE=testproject
		OPENSHIFT_BUILD_SOURCE=https://github.com/openshift/ruby-ex.git
		OPENSHIFT_BUILD_REFERENCE=master
		OPENSHIFT_BUILD_COMMIT=855ab2de53ff897a19e1055f7554c64d19e02c50
		PATH=/opt/app-root/src/bin:/opt/app-root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
		container=oci
		STI_SCRIPTS_URL=image:///usr/libexec/s2i
		STI_SCRIPTS_PATH=/usr/libexec/s2i
		HOME=/opt/app-root/src
		BASH_ENV=/opt/app-root/etc/scl_enable
		ENV=/opt/app-root/etc/scl_enable
		PROMPT_COMMAND=. /opt/app-root/etc/scl_enable
		RUBY_VERSION=2.3
----
<1> This is a special annotation that was put in place by the container scan
<2> This denotes which policy was used for the failed scan
<3> Where did the image come from? link:https://github.com/projectatomic/ContainerApplicationGenericLabels[This and more labels are explained here]
<4> What builder image was used to create this image?
<5> The docker name of the builder image
<6> The release version of the used builder image

* _Labels_: There are no OpenShift Labels on this object. If there were, you could use the -l "label=value" Option with the oc command line tool to select this object. As it is here, you can only use the long id that is found in the name field.

* _Annotations_: There are special annotations on this image. The highlighted annotations *images.openshift.io/deny-execution=true* and 
*security.manageiq.org/failed-policy=openscap policy* are put into the image metadata by the security scan that we triggered in CloudForms. These annotations to document that there were "important" security issues found. You can the annotations on the OpenShift side as we will later.

* _Docker Labels_: There are labels on the image that speak for it's origin. Look at vendor, name and release. The special OpenShift Label io.openshift.build.image notes the parent image. The *io.openshift.build.image* tells us that the rthest container image was built using the *rhscl/ruby-23-rhel7* builder image from the Red Hat Registry. Further down you find the *release* label that tells us that we used a builder image that was tagged with the docker label release=6.7.

//Open TODO Explain S2I - Source 2 Image Builds.

Lets find the image in the link:https://access.redhat.com/containers[Red Hat Container Catalog^]. Our Red Hat Container Catalog provides detailed information about the images that we as Red Hat provide. We do document known issues and the fixes once they become available. We recently added a "Health Index" to deliver an easy first impression about the freshness of an image. 

. Got to link:https://access.redhat.com/containers[https://access.redhat.com/containers^].
+
. Fill out the search field, put in *rhscl/ruby-23* and hit return. 
+
You can see that we do have a newer release available than 6.7. Back when I did this, it was 6.8 as you can see in the screen shot below. You will need to click on "Tags" to get to that same view.
+
image::http://people.redhat.com/~llange/labimg/RHCC-ruby-23-with-tags.png[Red Hat Container Catalog Ruby 2.3 Image Tags Tab^]
+
. Click the tag "link:https://access.redhat.com/containers/#/registry.access.redhat.com/rhscl/ruby-23-rhel7/images/2.3-6.7[2.3-6.7^]" in your list to get to the details about the builder image that we were using.

image::http://people.redhat.com/~llange/labimg/RHCC-ruby-23-67-details.png[Red Hat Container Calalog Ruby 2.3-6.7 Details^]

Lets take a closer look at the details in the 2.3-6.7 release of the rhscl/ruby-23-rhel7 builder image. The screen shot above lists the health index as B. It has an explanation on the side what this means. 

[NOTE]
.Health Index Level B
====
This image is affected by Critical ( no older than 7 days ) or Important ( no older than 30 days ) security updates
====

Also note that there is an update builder image available that fixes issues found in the release 6.7. It might be the case that not all issues are fixed in the latest available image. Red Hat is building new container images in a scheduled fashion. That is why we might not have a certain fix in the latest image.  We do divert from our scheduled build and do async updates for critical updates only.

=== Repair known Security Issues 

We just found out that there is a newer version of the ruby builder image available in the Red Hat Registry. Lets update the s2i builder Image to the latest available version to fix security issues. Images are managed in ImageStreams in OpenShift. So lets take a look at the ruby ImageStream before we go and let OpenShift get the newest version of the ruby builder image.

----
$ oc get is ruby -n openshift
NAME      DOCKER REPO                          TAGS                         UPDATED
ruby      172.30.120.134:5000/openshift/ruby   2.2,2.0,latest + 1 more...   3 weeks ago
----

Or with a lot more detail :

[subs=+macros]
----
$ oc describe is ruby -n openshift
Name:			ruby
Namespace:		openshift
Created:		3 weeks ago
Labels:			<none>
Annotations:		openshift.io/display-name=Ruby
			openshift.io/image.dockerRepositoryCheck=2017-05-16T13:09:06Z
Docker Pull Spec:	172.30.120.134:5000/openshift/ruby
pass:quotes[*Unique Images:		3*]
Tags:			4

2.3 (latest)
  tagged from registry.access.redhat.com/rhscl/ruby-23-rhel7:latest

  Build and run Ruby 2.3 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/blob/master/2.3/README.md.
  Tags: builder, ruby
  Supports: ruby:2.3, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

pass:quotes[  * *registry.access.redhat.com/rhscl/ruby-23-rhel7@sha256:4b496b8b4d306badbea387f790004f867ca774526c17fb0fffdc88d58384c495*]
      3 weeks ago

2.2
  tagged from registry.access.redhat.com/rhscl/ruby-22-rhel7:latest

  Build and run Ruby 2.2 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/tree/master/2.2/README.md.
  Tags: builder, ruby
  Supports: ruby:2.2, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

  * registry.access.redhat.com/rhscl/ruby-22-rhel7@sha256:f8b0adc1bdb409e0cfbaa39870077c4944eb52b8e222551ef3146eddf1c9e6cb
      3 weeks ago

2.0
  tagged from registry.access.redhat.com/openshift3/ruby-20-rhel7:latest

  Build and run Ruby 2.0 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/tree/master/2.0/README.md.
  Tags: hidden, builder, ruby
  Supports: ruby:2.0, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

  * registry.access.redhat.com/openshift3/ruby-20-rhel7@sha256:9cfdf4b811ace13d4c555335b249ab831832a384113035512abc9d4d5cc59716
      3 weeks ago
-
----

Notice the bold lines above. We do have *3 unique images* referenced by the ruby ImageStream currently. There is only one image listed for the tag 2.3.

==== Update the Builder Image

* The command below will update the ruby ImageStream and load the latest container image tagged ruby:2.3 from the Red Hat Registry.
* An update to the ruby ImageStream will have an effect for you rtest application. Your BuildConfig for the rtest application is setup to watch the openshift/ruby:2.3 image stream for new Images. The update will trigger a new s2i build.
* The result of that build will be a new version of your rtest application in container image format. 
* This will be pushed into the internal OpenShift registry. 
* The DeployementConfig of your rtest application watches the rtest ImageStream for new versions and will trigger a new deployment in turn. 

*You can watch all of this happening in your environment if you are quick enough*

[subs=+macros]
----
$ oc import-image ruby:2.3 -n openshift
The import completed successfully.

Name:			ruby
Namespace:		openshift
Created:		3 weeks ago
Labels:			<none>
Annotations:		openshift.io/display-name=Ruby
			openshift.io/image.dockerRepositoryCheck=2017-06-13T11:19:06Z
Docker Pull Spec:	172.30.120.134:5000/openshift/ruby
pass:quotes[*Unique Images:		4*]
Tags:			4

2.3 (latest)
  tagged from registry.access.redhat.com/rhscl/ruby-23-rhel7:latest

  Build and run Ruby 2.3 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/blob/master/2.3/README.md.
  Tags: builder, ruby
  Supports: ruby:2.3, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

pass:quotes[  * *registry.access.redhat.com/rhscl/ruby-23-rhel7@sha256:3539e468222542cbea0c127927db191c2bd823e134ab241de971c2f14fed5fc7
      Less than a second ago*]
    registry.access.redhat.com/rhscl/ruby-23-rhel7@sha256:4b496b8b4d306badbea387f790004f867ca774526c17fb0fffdc88d58384c495
      3 weeks ago

2.2
  tagged from registry.access.redhat.com/rhscl/ruby-22-rhel7:latest

  Build and run Ruby 2.2 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/tree/master/2.2/README.md.
  Tags: builder, ruby
  Supports: ruby:2.2, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

  * registry.access.redhat.com/rhscl/ruby-22-rhel7@sha256:f8b0adc1bdb409e0cfbaa39870077c4944eb52b8e222551ef3146eddf1c9e6cb
      3 weeks ago

2.0
  tagged from registry.access.redhat.com/openshift3/ruby-20-rhel7:latest

  Build and run Ruby 2.0 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-ruby-container/tree/master/2.0/README.md.
  Tags: hidden, builder, ruby
  Supports: ruby:2.0, ruby
  Example Repo: https://github.com/openshift/ruby-ex.git

  * registry.access.redhat.com/openshift3/ruby-20-rhel7@sha256:9cfdf4b811ace13d4c555335b249ab831832a384113035512abc9d4d5cc59716
      3 weeks ago
----

Notice that we now have 4 unique images and that there is a new image for the 2.3 tag that was synced less than a second ago.

==== Watch the new Build 

.See the build in OpenShift
====
. Go to the Overview Page of the "Test Project" and see the new build running. ( you need to be quick )

image::http://people.redhat.com/~llange/labimg/Image-Update-rtest-build-triggered.png[A new build of the rtest container image was triggerd]

. Watch the build on the the command line :

[subs=+macros]
----
# oc get bc rtest
NAME      TYPE      FROM      LATEST
rtest     Source    Git       2

[root@master ~]# oc describe bc
Name:		rtest
Namespace:	testproject
Created:	About an hour ago
Labels:		app=rtest
Annotations:	openshift.io/generated-by=OpenShiftNewApp
Latest Version:	2

Strategy:	Source
URL:		https://github.com/openshift/ruby-ex
From Image:	ImageStreamTag openshift/ruby:2.3 <1>
Output to:	ImageStreamTag rtest:latest

Build Run Policy:	Serial
Triggered by:		Config, ImageChange <2>
Webhook GitHub:
	URL:	https://master.example.com:8443/oapi/v1/namespaces/testproject/buildconfigs/rtest/webhooks/bIqI1y4ETX7uADNm-PMo/github
Webhook Generic:
	URL:		https://master.example.com:8443/oapi/v1/namespaces/testproject/buildconfigs/rtest/webhooks/aeS1J1OTCInI4Fv4kQMh/generic
	AllowEnv:	false

Build		Status		Duration	Creation Time
rtest-2 	complete 	2m29s 		2017-06-13 07:19:06 -0400 EDT
rtest-1 	complete 	2m17s 		2017-06-13 06:24:44 -0400 EDT
----

<1> This is the reference to the *openshift/ruby:2.3* ImageStream
<2> Here you see what kind of triggers are configured for this buildconfig

This is the BuildConfig that is watching the builder image ImageStream and start a new build automatically if an image change is detected.
====

==== Inspect the new Deployment

The new build did put a new image into the internal registry. There is an rtest ImageStream in the testproject that is used by the rrtest DeploymentConfig to watch for ImageChanges and trigger new deployements.

. Look up the rtest deployment in the WebUI : 
.. Select andrews "Test Project"
.. Select "Applications"
.. Click "Deployements" in the sub menu
.. Click "rtest" in the list of deployments
+
image::http://people.redhat.com/~llange/labimg/Image-Update-rtest-deployment-3.png[rtest deployment]
+
. Look at your rtest DeploymentConfig on the cmd :
+
----
[root@master ~]# oc describe dc
Name:		rtest
Namespace:	testproject
Created:	2 hours ago
Labels:		app=rtest
Annotations:	openshift.io/generated-by=OpenShiftNewApp
Latest Version:	3
Selector:	app=rtest,deploymentconfig=rtest
Replicas:	1
Triggers:	Config, Image(rtest@latest, auto=true)
Strategy:	Rolling
Template:
  Labels:	app=rtest
		deploymentconfig=rtest
  Annotations:	openshift.io/generated-by=OpenShiftNewApp
  Containers:
   rtest:
    Image:			172.30.120.134:5000/testproject/rtest@sha256:828d41ae8c7044026732d2092734b312a27044241c23238f3a01525ad5a606c2
    Port:			8080/TCP
    Liveness:			http-get http://:8080/health delay=0s timeout=1s period=10s #success=1 #failure=3
    Volume Mounts:		<none>
    Environment Variables:	<none>
  No volumes.

Deployment #3 (latest):
	Name:		rtest-3
	Created:	45 minutes ago
	Status:		Complete
	Replicas:	1 current / 1 desired
	Selector:	app=rtest,deployment=rtest-3,deploymentconfig=rtest
	Labels:		app=rtest,openshift.io/deployment-config.name=rtest
	Pods Status:	1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Deployment #2:
	Created:	about an hour ago
	Status:		Complete
	Replicas:	0 current / 0 desired
Deployment #1:
	Created:	2 hours ago
	Status:		Complete
	Replicas:	0 current / 0 desired
...

----
+
Both outputs above display that you are running on deployment No. 3 now. You can see in the Web UI that the last deployment was triggered by an image change. To see the same information on the command line you would need to take a look at the oc rollout command :
+
----
# oc rollout --help
...

# oc rollout history dc/rtest
deploymentconfigs "rtest"
REVISION	STATUS		CAUSE
1		Complete	image change
2		Complete	config change
3		Complete	image change

----
+
For more information about Deployments take a look at the link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#deployments[OpenShift Developer Guide Deployements Chapter].

If you know recent Kubernetes versions, you will have come across the Deployment Object. The OpenShift DeploymentConfig Objects are far more advanced than the Kubernetes Deployements. Read about the limitations link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-kubernetes-deployments-support[here]. You can expect the Kubernetes Deployments to get more and more features as we are upstreaming the work done in OpenShift.

==== Image Details (again) 

Which release of the ruby builder image was used to build the latest version of the rtest container image? You can check this in 3 different places, look for the release label in one of them :

. In CloudForms - Compute -> Containers -> Conainer Images - search rtest
. In the link:https://registry-console-default.cloudapps.example.com/registry[Registry Console]
. on the command line through oc describe is rtest and oc describe images $IMID 

----
# oc describe is rtest -n testproject | grep sha | head -1 
  * 172.30.120.134:5000/testproject/rtest@sha256:828d41ae8c7044026732d2092734b312a27044241c23238f3a01525ad5a606c2
# oc describe images sha256:828d41ae8c7044026732d2092734b312a27044241c23238f3a01525ad5a606c2 | grep release
		release=6.8
----

==== Rescan the new container images

Go to the CloudForms interface and schedule another image scam for the newly created rtest image. Notice that you will need to know your sha256 ID to identify the correct images quickly. In my case this sha256:828d... 

. Find the new rtest Image
.. Compute -> Containers -> Container Images -> Search "rtest"
.. Click the correct Container Image identified by the sha256:...
. Assign the OpenSCAP Policy Profile
.. Open the Menu "Policy" 
.. Click "Manage Policies"
.. Select check box "OpenSCAP profile"
.. Click save
. Schedule Container Scan
.. Click "Configuration" Menu
.. Select "Perform SmartState Analysis"
.. Confirm "Perform SmartState Analysis on this item"
.Wait a few minutes and reload the page. If the scan does not start at all, close your browser completely, start it again and schedlue the smart state analysis again. 
.Load the OpenSCAP html information by clicking the "OpenSCAP HTML" line
.. select to open in your browser
.. scoll down and deselect the check box "pass"

image::http://people.redhat.com/~llange/labimg/OpenSCAP-scan-results-in-cf-rtest-2.png[]

Notice that we have fewer issues then before. The false finding with the aarch64 issue is still in and you can continue to ignore it. The link:https://access.redhat.com/errata/RHSA-2017:1308[RHSA-2017:1308] issue is gone. The nss security bug link:https://access.redhat.com/errata/RHSA-2017:1365[RHSA-2017-1365] is still present as was to be expected. We saw that this issue was not fixed with the link:https://access.redhat.com/containers/#/registry.access.redhat.com/rhscl/ruby-23-rhel7/images/2.3-6.8[6.8 release of our rhscl/ruby-23-rhel7] builder image when we looked it up in the link:https://access.redhat.com/containers[Red Hat Container Catalog].

==== Get the latest ruby builder image automatically 

Could OpenShift get the latest builder image as soon as it becomes available? Yes it can!

----
# oc tag is ruby:2.3 -n openshift --scheduled=true
Tag ruby:2.3 set to import is periodically.
----

This will set the ImportPolicy on the Image in the ImageStream Ruby that is tagged with 2.3. OpenShift will fetch new versions of this builder image every 15 min if the are available. The interval can be set in the master-config.yaml. The keyword is scheduledImageImportMinimumIntervalSeconds and defaults to 900 if it is not specified. 

=== Continued Security in Container Environments 

This section will show you how you can examine your Container Images on a regular basis, and how you OpenShift can consume this information to prevent execution of additional workloads when they are found to be vulnerable.

==== Create a Schedule for regular Security Checks  

It is usually not sufficient to trigger container scans manually. If you build and check an image today and it is found to be good, this can change tomorrow or in a week or a year as new vulnerabilities are discovered. To address this, you want to schedule security scans at regular interval. You can do this with CloudForms. 

. First assign the OpenSCAP Policy not to a single image, but on the Container Provider Level.
.. Go to Compute -> Container -> Providers
.. Select the "OpenShift Cluster 1 Provider" checkbox
.. Click the Policy Button
.. Click "Manage Policies"
+
image::http://people.redhat.com/~llange/labimg/CloudForms-Manage-Policies-for-Providers.png[Manage Provider Policies] 
+
.. Select the "OpenSCAP profile" checkbox
+
image::http://people.redhat.com/~llange/labimg/CloudForms-Manage-Policies-for-Providers-OpenSCAP.png[Manage Provider Policies assign OpenSCAP Profile]
+
.. Click the "Save" button.

. Create a Schedule to scan all images in your internal registry.
.. Open the EVM Menu on the upper right corner of the CloudForms Interfach
.. Click "Configuration"
+
image::http://people.redhat.com/~llange/labimg/CloudForms-Add-Schedule-1.png[]
+
.. Make sure to select "Schedules" in the Setting Part of the Accordion. 
.. Open the "Configuration" Menu
.. Click "Add Schedule"
+
image::http://people.redhat.com/~llange/labimg/CloudForms-Add-Schedule-2.png[]
+
.. Fill in the Details for you Schedule as in the screen shot below. But select a time ~10 min from now if you want to see CloudForms scheduling scans.
+
image::http://people.redhat.com/~llange/labimg/CloudForms-Create-Scanning-Schedule-3.png[]
+
.. Click the "Add" button to add the schedule.

Now wait for that schedule to start scanning all your container images.

==== Prevent the starting of vulnerable Workloads

It is possible to use the annotations that the Container Scan puts into the Container Image Metadata on the OpenShift side to prevent starting more vulnerable workloads. You can consult the documentation for more background :

* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/container_security_guide/#controlling-pod-execution[Documentation on Controlling Pod Executon]

* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/cluster_administration/#admin-guide-image-policy[ImagePolicy Documentation]

. Before we implement this safe guard, verify that you can scale up your rtest application. This can be done easily from the Web UI by increasing the number of pods in the Project Overview, or in the DeploymentConfig. 
+
.Scale rtest in the Web UI Project Overview 
image::http://people.redhat.com/~llange/labimg/OpenShift-Scaling-rtest-Project-Overview-level.png[]
+
.Scale rtest in the Deployment View
image::http://people.redhat.com/~llange/labimg/OpenShift-Scaling-rtest-dc-level.png[]
+
. You can do the same on the command line with :
+
.Scale rtest in the command line
----
# oc get rc 
NAME      DESIRED   CURRENT   READY     AGE
rtest-1   0         0         0         19h
rtest-2   0         0         0         19h
rtest-3   0         0         0         19h
rtest-4   2         2         2         5m
----
+
----
# oc scale dc/rtest --replicas=3
deploymentconfig "rtest" scaled
----
+
----
# oc get rc 
NAME      DESIRED   CURRENT   READY     AGE
rtest-1   0         0         0         20h
rtest-2   0         0         0         19h
rtest-3   0         0         0         19h
rtest-4   3         3         3         7m
----
+
----
# oc get pods
[root@master ~]# oc get pods
NAME            READY     STATUS      RESTARTS   AGE
rtest-1-build   0/1       Completed   0          20h
rtest-2-build   0/1       Completed   0          19h
rtest-4-1mzg0   1/1       Running     0          2m
rtest-4-crlfp   1/1       Running     0          5m
rtest-4-s6bmz   1/1       Running     0          7m
----
+
You can see above that the rtest-4 deployment was scaled to 3 pods successfully. 
+
We can use the ImagePolicy settings in the `master-config.yaml` of the OpenShift Master to instruct OpenShift not to start any workloads that have a certain annotation. Already running workloads would not be affected by this unless you scale them down. Scaling up counts as starting new workloads in this case. 
+
. Make sure the following settings are found in your master-config.yaml. Be sure to get the indentation right.
+
.ImagePolicy in /etc/origin/master/master-config.yaml
[subs=+macros]
----
admissionConfig:
  pluginConfig:   
pass:quotes[    *openshift.io/ImagePolicy:
      configuration:
        kind: ImagePolicyConfig
        apiVersion: v1
        resolveImages: AttemptRewrite
        executionRules:
        - name: execution-denied
          onResources:
          - resource: pods
          - resource: builds
          reject: true
          matchImageAnnotations:
          - key: images.openshift.io/deny-execution
            value: "true"
          skipOnResolutionFailure: true
        - name: allow-images-from-internal-registry
          # allows images from the internal registry and tries to resolve them
          onResources:
          - resource: pods
          - resource: builds
          matchIntegratedRegistry: true
        - name: allow-images-from-dockerhub
          onResources:
          - resource: pods
          - resource: builds
          matchRegistries:
          - docker.io*]
    BuildDefaults:
...
----
+
. Restart the atomic-openshift-master service
+
----
$ systemctl restart atomic-openshift-master
----
+
. try scaling up your rtest application now.
+
.Note that you fail and scaling seems to be stuck 
image::http://people.redhat.com/~llange/labimg/OpenShift-Scaling-rtest-Project-Overview-level-not-working.png[]
+
. Lets us dig for an error message about what is going on
.. Go to Monitoring in the Web UI
.. Find the Events section on the top left
.. Click "View Details"
+
.This tells why you can't scale up any more
image::http://people.redhat.com/~llange/labimg/OpenShift-Scaling-rtest-prevented-event-view.png[]

The message reads : "Forbidden: this image is prohibited by policy".

== The End

Thanks a lot for attending the *Manage your Conatainers with OpenShift* Lab, we hope you enjoyed it.

Have a good day :-)